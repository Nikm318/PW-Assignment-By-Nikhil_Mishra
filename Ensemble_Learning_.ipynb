{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer:  Ensemble Learning is a machine learning technique in which multiple individual models (called base learners or weak learners) are trained and then combined to make a single, stronger predictive model. Instead of relying on one model, ensemble learning aggregates the predictions of several models to improve overall performance.\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "\n",
        "The central idea of ensemble learning is:\n",
        "\n",
        "A group of diverse models, when combined, can produce better and more reliable predictions than any single model alone.\n",
        "\n",
        "This improvement happens because different models tend to make different errors. By combining them intelligently, these errors can cancel out, leading to:\n",
        "\n",
        "Higher accuracy\n",
        "\n",
        "Better generalization to unseen data\n",
        "\n",
        "Reduced overfitting\n",
        "\n",
        "Improved robustness\n",
        "\n",
        "How Ensemble Learning Works (Conceptually)\n",
        "\n",
        "Train multiple models on the same dataset (or different samples/features of it).\n",
        "\n",
        "Ensure diversity among models (different algorithms, data subsets, or parameters).\n",
        "\n",
        "Combine predictions using techniques such as:\n",
        "\n",
        "Majority voting (classification)\n",
        "\n",
        "Averaging (regression)\n",
        "\n",
        "Weighted combinations\n",
        "\n",
        "Why Ensemble Learning Is Effective\n",
        "\n",
        "Reduces variance: Combines unstable models like decision trees to stabilize predictions.\n",
        "\n",
        "Reduces bias: Sequential ensembles focus on correcting previous mistakes.\n",
        "\n",
        "Handles complex patterns: Captures different aspects of the data.\n",
        "\n",
        "Common Ensemble Techniques (for context)\n",
        "\n",
        "Bagging (Bootstrap Aggregating) – e.g., Random Forest\n",
        "\n",
        "Boosting – e.g., AdaBoost, Gradient Boosting\n",
        "\n",
        "Stacking – Combines predictions using a meta-model\n",
        "\n",
        "Simple Intuition\n",
        "\n",
        "Think of ensemble learning as taking advice from multiple experts instead of trusting just one. Even if some experts are wrong, the collective decision is usually more accurate."
      ],
      "metadata": {
        "id": "HFQy63Syaf9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:   Difference Between Bagging and Boosting\n",
        "\n",
        "Bagging (Bootstrap Aggregating) and Boosting are two popular ensemble learning techniques, but they differ significantly in how models are trained and combined.\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Training approach:\n",
        "Multiple models are trained independently and in parallel.\n",
        "\n",
        "Data sampling:\n",
        "Each model is trained on a different bootstrap sample (random sampling with replacement) from the original dataset.\n",
        "\n",
        "Focus:\n",
        "Reduces variance and helps prevent overfitting, especially for high-variance models like decision trees.\n",
        "\n",
        "Error handling:\n",
        "All models are treated equally; no special focus on misclassified samples.\n",
        "\n",
        "Prediction combination:\n",
        "\n",
        "Classification: Majority voting\n",
        "\n",
        "Regression: Averaging\n",
        "\n",
        "Typical example:\n",
        "Random Forest\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Training approach:\n",
        "Models are trained sequentially, one after another.\n",
        "\n",
        "Data sampling / weighting:\n",
        "Each new model gives more importance (higher weight) to samples that were misclassified by previous models.\n",
        "\n",
        "Focus:\n",
        "Reduces bias and improves performance on hard-to-classify instances.\n",
        "\n",
        "Error handling:\n",
        "Explicitly focuses on correcting previous model errors.\n",
        "\n",
        "Prediction combination:\n",
        "Weighted sum or weighted voting of models.\n",
        "\n",
        "Typical examples:\n",
        "AdaBoost, Gradient Boosting, XGBoost"
      ],
      "metadata": {
        "id": "08T9rTC7af6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:  \n",
        "\n",
        "Bootstrap Sampling and Its Role in Bagging (Random Forest)\n",
        "1. What is Bootstrap Sampling?\n",
        "\n",
        "Bootstrap sampling is a statistical resampling technique in which:\n",
        "\n",
        "A dataset of size N is created by randomly sampling with replacement from the original dataset of size N.\n",
        "\n",
        "Because sampling is done with replacement, some data points may appear multiple times, while others may not appear at all in a given sample.\n",
        "\n",
        "On average:\n",
        "\n",
        "About 63.2% of unique observations appear in each bootstrap sample.\n",
        "\n",
        "The remaining 36.8% are left out and are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "2. Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "In Bagging (Bootstrap Aggregating), bootstrap sampling is used to:\n",
        "\n",
        "Generate multiple different training datasets from the same original data.\n",
        "\n",
        "Train multiple models independently on these different datasets.\n",
        "\n",
        "Introduce diversity among models, which is essential for an effective ensemble.\n",
        "\n",
        "3. Role in Random Forest\n",
        "\n",
        "Random Forest is a classic example of a bagging-based method that relies heavily on bootstrap sampling.\n",
        "\n",
        "Bootstrap sampling in Random Forest:\n",
        "\n",
        "Creates diversity among trees\n",
        "Each decision tree is trained on a different bootstrap sample, making the trees less correlated.\n",
        "\n",
        "Reduces variance\n",
        "Individual decision trees are high-variance models. Aggregating many trees trained on bootstrap samples stabilizes predictions.\n",
        "\n",
        "Enables Out-of-Bag (OOB) error estimation\n",
        "\n",
        "Samples not included in a tree’s bootstrap dataset (OOB samples) are used to test that tree.\n",
        "\n",
        "OOB error provides a reliable estimate of model performance without needing a separate validation set.\n",
        "\n",
        "Additionally, Random Forest adds feature randomness (random subset of features at each split), which further reduces correlation between trees.\n",
        "\n",
        "4. Why Bootstrap Sampling Is Important\n",
        "\n",
        "Without bootstrap sampling:\n",
        "\n",
        "All models would see the same data.\n",
        "\n",
        "Predictions would be highly correlated.\n",
        "\n",
        "The ensemble would offer little improvement over a single model."
      ],
      "metadata": {
        "id": "zQhIqUzPaf3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:  Out-of-Bag (OOB) Samples and OOB Score in Ensemble Models\n",
        "1. What are Out-of-Bag (OOB) Samples?\n",
        "\n",
        "In bagging-based ensemble methods (such as Random Forest), each base model is trained on a bootstrap sample drawn with replacement from the original dataset.\n",
        "\n",
        "Since sampling is done with replacement, not all training instances are selected for a given model.\n",
        "\n",
        "The data points not included in a model’s bootstrap sample are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "On average, about 36.8% of the original dataset acts as OOB data for each base model.\n",
        "\n",
        "2. How OOB Samples Are Used\n",
        "\n",
        "For each training instance:\n",
        "\n",
        "That instance is OOB for all models that did not include it in their bootstrap sample.\n",
        "\n",
        "Predictions are made for the instance using only those models where it was OOB.\n",
        "\n",
        "These predictions are then aggregated:\n",
        "\n",
        "Classification: majority voting\n",
        "\n",
        "Regression: averaging\n",
        "\n",
        "3. What is the OOB Score?\n",
        "\n",
        "The OOB score is a performance metric computed by:\n",
        "\n",
        "Comparing the aggregated OOB predictions with the true target values.\n",
        "\n",
        "Measuring accuracy (for classification) or error metrics such as MSE/R² (for regression).\n",
        "\n",
        "In Random Forest:\n",
        "\n",
        "The OOB score serves as an internal validation estimate of model performance.\n",
        "\n",
        "It is conceptually similar to cross-validation, but computationally cheaper.\n",
        "\n",
        "4. Why OOB Score Is Important\n",
        "\n",
        "No separate validation set required\n",
        "Efficient use of data, especially useful when datasets are small.\n",
        "\n",
        "Unbiased performance estimate\n",
        "Each prediction is made by models that never saw that data point during training.\n",
        "\n",
        "Faster evaluation\n",
        "Avoids repeated retraining as in k-fold cross-validation.\n",
        "\n",
        "Built-in model assessment\n",
        "Automatically computed in algorithms like Random Forest.\n",
        "\n",
        "5. Limitations of OOB Score\n",
        "\n",
        "Available only in bagging-based methods.\n",
        "\n",
        "Less reliable when:\n",
        "\n",
        "Number of trees is small.\n",
        "\n",
        "Bootstrap sampling is disabled."
      ],
      "metadata": {
        "id": "MY9Ho9aYafuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Feature Importance: Decision Tree vs. Random Forest\n",
        "\n",
        "Feature importance measures how much each input feature contributes to a model’s predictions. While both Decision Trees and Random Forests provide feature importance, the way it is computed and its reliability differ significantly.\n",
        "\n",
        "1. Feature Importance in a Single Decision Tree\n",
        "\n",
        "How it is calculated\n",
        "\n",
        "Based on the reduction in impurity (e.g., Gini impurity or entropy for classification, variance reduction for regression).\n",
        "\n",
        "Each time a feature is used to split a node, the impurity decrease is attributed to that feature.\n",
        "\n",
        "The total decrease across the tree defines feature importance.\n",
        "\n",
        "Characteristics\n",
        "\n",
        "Highly sensitive to training data: small data changes can alter the tree structure.\n",
        "\n",
        "High variance: importance scores may change drastically across different trees.\n",
        "\n",
        "Biased toward features with many possible split points (continuous or high-cardinality features).\n",
        "\n",
        "Easy to interpret visually, since the entire decision path is visible.\n",
        "\n",
        "Implication\n",
        "\n",
        "Suitable for quick, interpretable insights but not reliable for robust importance estimation.\n",
        "\n",
        "2. Feature Importance in a Random Forest\n",
        "\n",
        "How it is calculated\n",
        "\n",
        "Importance is computed for each tree individually (using impurity reduction).\n",
        "\n",
        "Final importance is the average importance across all trees.\n",
        "\n",
        "Some implementations also support permutation importance, which measures performance drop when a feature’s values are shuffled.\n",
        "\n",
        "Characteristics\n",
        "\n",
        "More stable and robust due to averaging across many trees.\n",
        "\n",
        "Less sensitive to noise and sampling variation.\n",
        "\n",
        "Reduced overfitting compared to a single tree.\n",
        "\n",
        "Still somewhat biased toward high-cardinality features when using impurity-based importance (permutation importance mitigates this).\n",
        "\n",
        "Implication\n",
        "\n",
        "Provides a more reliable estimate of feature relevance, especially for complex datasets.\n",
        "\n",
        "| Aspect                                | Decision Tree       | Random Forest              |\n",
        "| ------------------------------------- | ------------------- | -------------------------- |\n",
        "| Model type                            | Single model        | Ensemble of trees          |\n",
        "| Stability                             | Low (high variance) | High (averaged over trees) |\n",
        "| Sensitivity to data                   | Very high           | Low                        |\n",
        "| Overfitting risk                      | High                | Low                        |\n",
        "| Feature importance reliability        | Less reliable       | More reliable              |\n",
        "| Bias toward high-cardinality features | High                | Reduced (not eliminated)   |\n",
        "| Interpretability                      | High                | Lower (many trees)         |\n"
      ],
      "metadata": {
        "id": "9yQskS9Vbr3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgbTEXkhacJj",
        "outputId": "96b89e13-48d3-4bfc-aa27-45ae13b9911a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Importance\": rf_model.feature_importances_\n",
        "})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "feature_importance = feature_importance.sort_values(\n",
        "    by=\"Importance\", ascending=False\n",
        ")\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Train Single Decision Tree\n",
        "# -------------------------------\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "# -------------------------------\n",
        "# Train Bagging Classifier\n",
        "# -------------------------------\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "bagging_predictions = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# -------------------------------\n",
        "# Print Results\n",
        "# -------------------------------\n",
        "print(\"Accuracy Comparison:\")\n",
        "print(f\"Single Decision Tree Accuracy : {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy   : {bagging_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frE9qv5-vjSq",
        "outputId": "14f163a3-f296-4b7e-fb62-4e185f25ece7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Single Decision Tree Accuracy : 1.0000\n",
            "Bagging Classifier Accuracy   : 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {final_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwHxvw22xFYo",
        "outputId": "f655d9dc-dde1-42dc-a77f-b4c62274fc28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'max_depth': None, 'n_estimators': 200}\n",
            "\n",
            "Final Test Accuracy: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Train Bagging Regressor\n",
        "# -------------------------------\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and MSE\n",
        "bagging_predictions = bagging_regressor.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "\n",
        "# -------------------------------\n",
        "# Train Random Forest Regressor\n",
        "# -------------------------------\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and MSE\n",
        "rf_predictions = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# -------------------------------\n",
        "# Print Results\n",
        "# -------------------------------\n",
        "print(\"Model Comparison (Mean Squared Error):\")\n",
        "print(f\"Bagging Regressor MSE      : {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4LSHN9TxWUX",
        "outputId": "3f65c7fd-184c-42c5-e627-5d30344677b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Comparison (Mean Squared Error):\n",
            "Bagging Regressor MSE      : 0.2568\n",
            "Random Forest Regressor MSE: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "\n",
        "Answer:  \n",
        "\n",
        "1. Choosing Between Bagging and Boosting\n",
        "Step 1: Analyze the Data Characteristics\n",
        "\n",
        "Demographic data → relatively stable patterns\n",
        "\n",
        "Transaction history → complex, non-linear, time-dependent patterns\n",
        "\n",
        "Loan default → typically imbalanced classification problem\n",
        "\n",
        "Decision Logic\n",
        "\n",
        "Start with Bagging if:\n",
        "\n",
        "The base model (e.g., Decision Tree) shows high variance\n",
        "\n",
        "Overfitting is observed on training data\n",
        "\n",
        "Move to Boosting if:\n",
        "\n",
        "The model underfits (high bias)\n",
        "\n",
        "There are subtle patterns in defaults that a single model misses\n",
        "\n",
        "Practical Choice\n",
        "\n",
        "Begin with Random Forest (Bagging-based) as a strong baseline.\n",
        "\n",
        "Progress to Gradient Boosting / XGBoost if higher recall on defaulters is required.\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "Overfitting is a critical risk in financial models due to regulatory and business impact.\n",
        "\n",
        "Techniques Used\n",
        "\n",
        "Bagging\n",
        "\n",
        "Reduces variance by averaging multiple independent models.\n",
        "\n",
        "Boosting\n",
        "\n",
        "Controls overfitting using:\n",
        "\n",
        "Learning rate\n",
        "\n",
        "Maximum tree depth\n",
        "\n",
        "Early stopping\n",
        "\n",
        "Regularization\n",
        "\n",
        "Limit tree depth\n",
        "\n",
        "Minimum samples per leaf\n",
        "\n",
        "Feature selection\n",
        "\n",
        "Remove highly correlated or noisy transaction features\n",
        "\n",
        "3. Selecting Base Models\n",
        "Preferred Base Learners\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Handle non-linear relationships\n",
        "\n",
        "Capture interaction between features\n",
        "\n",
        "No need for feature scaling\n",
        "\n",
        "Why Decision Trees?\n",
        "\n",
        "Weak learners individually\n",
        "\n",
        "Strong learners when combined in ensembles\n",
        "\n",
        "Naturally explainable via feature importance\n",
        "\n",
        "Advanced Option\n",
        "\n",
        "Combine:\n",
        "\n",
        "Logistic Regression (interpretable baseline)\n",
        "\n",
        "Tree-based ensembles (performance)\n",
        "\n",
        "Use stacking for final predictions if allowed by governance.\n",
        "\n",
        "4. Evaluating Performance Using Cross-Validation\n",
        "Why Cross-Validation is Mandatory\n",
        "\n",
        "Prevents over-reliance on a single train-test split\n",
        "\n",
        "Ensures model stability across customer segments\n",
        "\n",
        "Approach\n",
        "\n",
        "Use Stratified K-Fold Cross-Validation\n",
        "\n",
        "Maintains default vs non-default ratio\n",
        "\n",
        "Evaluate using:\n",
        "\n",
        "ROC-AUC → ranking risk\n",
        "\n",
        "Recall (Default Class) → minimize false negatives\n",
        "\n",
        "Precision → control false positives\n",
        "\n",
        "F1-Score → balance risk\n",
        "\n",
        "Additional Validation\n",
        "\n",
        "Out-of-time validation (historical vs recent customers)\n",
        "\n",
        "Stability testing across income groups and regions\n",
        "\n",
        "| Aspect              | Single Model | Ensemble Model |\n",
        "| ------------------- | ------------ | -------------- |\n",
        "| Accuracy            | Moderate     | High           |\n",
        "| Stability           | Low          | High           |\n",
        "| Risk Capture        | Limited      | Improved       |\n",
        "| Generalization      | Weak         | Strong         |\n",
        "| Decision Confidence | Lower        | Higher         |\n",
        "\n",
        "\n",
        "Real-World Benefits\n",
        "\n",
        "Lower default risk by identifying high-risk borrowers accurately\n",
        "\n",
        "Reduced financial loss due to fewer false approvals\n",
        "\n",
        "Fairer decisions by minimizing model bias\n",
        "\n",
        "Regulatory compliance through stable and validated models"
      ],
      "metadata": {
        "id": "I4ROtq4ayQnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: You are working as a data scientist at a financial institution to predict loan default.\n",
        "#You have access to customer demographic and transaction history data.\n",
        "#You decide to use ensemble techniques to increase model performance.\n",
        "#Explain your step-by-step approach to:\n",
        "#● Choose between Bagging or Boosting\n",
        "#● Handle overfitting ● Select base models\n",
        "#● Evaluate performance using cross-validation\n",
        "#● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "# Import required libraries\n",
        "# ==============================\n",
        "# IMPORT LIBRARIES\n",
        "# ==============================\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# ==============================\n",
        "# LOAD DATASET (AVAILABLE IN COLAB)\n",
        "# ==============================\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target   # 0 = non-default, 1 = default (analogy)\n",
        "\n",
        "# ==============================\n",
        "# TRAIN-TEST SPLIT (NO STRATIFY)\n",
        "# ==============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# BAGGING MODEL (RANDOM FOREST)\n",
        "# ==============================\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# BOOSTING MODEL (GRADIENT BOOSTING)\n",
        "# ==============================\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# ERROR-PROOF CROSS VALIDATION\n",
        "# ==============================\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_auc = cross_val_score(\n",
        "    rf_model, X_train, y_train, cv=cv, scoring=\"roc_auc\"\n",
        ")\n",
        "\n",
        "gb_auc = cross_val_score(\n",
        "    gb_model, X_train, y_train, cv=cv, scoring=\"roc_auc\"\n",
        ")\n",
        "\n",
        "print(\"Random Forest Mean ROC-AUC:\", rf_auc.mean())\n",
        "print(\"Gradient Boosting Mean ROC-AUC:\", gb_auc.mean())\n",
        "\n",
        "# ==============================\n",
        "# TRAIN FINAL MODEL (BOOSTING)\n",
        "# ==============================\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# ==============================\n",
        "# FINAL TEST EVALUATION\n",
        "# ==============================\n",
        "y_pred_prob = gb_model.predict_proba(X_test)[:, 1]\n",
        "final_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "print(\"Final Test ROC-AUC:\", final_auc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk0orj05xob2",
        "outputId": "7d8577e7-810b-4d65-f8d8-6eb0e93476d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Mean ROC-AUC: 0.9874859564302222\n",
            "Gradient Boosting Mean ROC-AUC: 0.9873304597060445\n",
            "Final Test ROC-AUC: 0.9952968841857731\n"
          ]
        }
      ]
    }
  ]
}