{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "\n",
        "Answer:  K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based (lazy learning) algorithm used for both classification and regression tasks. It makes predictions based on the similarity between a new data point and the existing labeled data.\n",
        "\n",
        "How KNN Works (Common Steps)\n",
        "\n",
        "Choose K – the number of nearest neighbors to consider.\n",
        "\n",
        "Compute distance between the new data point and all training points\n",
        "(commonly Euclidean distance, but Manhattan, Minkowski, or cosine distance may also be used).\n",
        "\n",
        "Select the K closest data points.\n",
        "\n",
        "Aggregate their outputs to make a prediction.\n",
        "\n",
        "KNN does not build an explicit model during training; all computation happens at prediction time.\n",
        "\n",
        "KNN for Classification\n",
        "\n",
        "In classification, KNN assigns a class label to a new data point based on the majority class among its K nearest neighbors.\n",
        "\n",
        "Example\n",
        "\n",
        "If K = 5 and the nearest neighbors have labels:\n",
        "\n",
        "Class A → 3 points\n",
        "\n",
        "Class B → 2 points\n",
        "\n",
        "The new data point is classified as Class A.\n",
        "\n",
        "Key Points\n",
        "\n",
        "Output: Discrete class\n",
        "\n",
        "Decision rule: Majority voting\n",
        "\n",
        "Can use distance-weighted voting, where closer neighbors have more influence.\n",
        "\n",
        "KNN for Regression\n",
        "\n",
        "In regression, KNN predicts a continuous value by averaging (or weighting) the values of the K nearest neighbors.\n",
        "\n",
        "Example\n",
        "\n",
        "If K = 4 and the target values of neighbors are:\n",
        "\n",
        "10, 12, 14, 16\n",
        "\n",
        "Predicted value =\n",
        "\n",
        "10+12+14+16\n",
        "\t​\n",
        "\n",
        "=13\n",
        "Key Points\n",
        "\n",
        "Output: Continuous value\n",
        "\n",
        "Decision rule: Mean or weighted mean\n",
        "\n",
        "Distance-weighted averages often improve accuracy.\n",
        "\n",
        "Choice of K\n",
        "\n",
        "Small K → Low bias, high variance (sensitive to noise)\n",
        "\n",
        "Large K → High bias, low variance (smoother predictions)\n",
        "\n",
        "Optimal K is usually chosen using cross-validation.\n",
        "\n",
        "Advantages of KNN\n",
        "\n",
        "Simple and intuitive\n",
        "\n",
        "No training phase\n",
        "\n",
        "Works well with small datasets\n",
        "\n",
        "Can handle multi-class classification naturally\n",
        "\n",
        "Limitations of KNN\n",
        "\n",
        "Computationally expensive at prediction time\n",
        "\n",
        "Sensitive to feature scaling (normalization is essential)\n",
        "\n",
        "Performance degrades with large datasets\n",
        "\n",
        "Sensitive to irrelevant features and noise"
      ],
      "metadata": {
        "id": "KNwX2H3DgXpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer: Curse of Dimensionality\n",
        "\n",
        "The Curse of Dimensionality refers to a set of problems that arise when working with high-dimensional data (i.e., data with a large number of features). As the number of dimensions increases, the feature space grows exponentially, causing data points to become increasingly sparse. This sparsity makes it difficult for distance-based algorithms like K-Nearest Neighbors (KNN) to work effectively.\n",
        "\n",
        "How It Affects KNN Performance\n",
        "\n",
        "KNN relies entirely on distance calculations to identify “nearest” neighbors. In high-dimensional spaces, this assumption breaks down in several ways:\n",
        "\n",
        "Distance Concentration\n",
        "\n",
        "In high dimensions, the distance between the nearest and farthest points becomes almost the same.\n",
        "\n",
        "As a result, nearest neighbors are no longer meaningfully close.\n",
        "\n",
        "This reduces KNN’s ability to distinguish between relevant and irrelevant neighbors.\n",
        "\n",
        "Reduced Similarity Meaning\n",
        "\n",
        "With many features, points tend to appear equally distant from each other.\n",
        "\n",
        "Distance metrics (like Euclidean distance) lose their discriminative power.\n",
        "\n",
        "KNN predictions become closer to random guessing.\n",
        "\n",
        "Increased Data Requirement\n",
        "\n",
        "To maintain meaningful neighborhood relationships, the dataset size must grow exponentially with the number of dimensions.\n",
        "\n",
        "In practice, this is often infeasible, leading to poor generalization.\n",
        "\n",
        "Higher Computational Cost\n",
        "\n",
        "More dimensions mean more distance calculations.\n",
        "\n",
        "This increases prediction time and memory usage, making KNN inefficient for large, high-dimensional datasets.\n",
        "\n",
        "Noise and Irrelevant Features\n",
        "\n",
        "High-dimensional data often contains irrelevant or noisy features.\n",
        "\n",
        "These features distort distance calculations, causing incorrect neighbor selection."
      ],
      "metadata": {
        "id": "jamZizFKgXmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:  Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving as much variance (information) as possible.\n",
        "\n",
        "Instead of selecting existing features, PCA creates new features, called principal components, which are linear combinations of the original variables.\n",
        "\n",
        "How PCA Works (Conceptually)\n",
        "\n",
        "Standardize the data (mean = 0, variance = 1).\n",
        "\n",
        "Compute the covariance matrix to understand feature relationships.\n",
        "\n",
        "Calculate eigenvectors and eigenvalues:\n",
        "\n",
        "Eigenvectors → directions of maximum variance (principal components)\n",
        "\n",
        "Eigenvalues → amount of variance captured by each component\n",
        "\n",
        "Sort components by descending eigenvalues.\n",
        "\n",
        "Select top K components that explain most of the variance.\n",
        "\n",
        "Project data onto these components to obtain reduced dimensions.\n",
        "\n",
        "Key Characteristics of PCA\n",
        "\n",
        "Unsupervised (does not use target variable)\n",
        "\n",
        "Reduces dimensionality while minimizing information loss\n",
        "\n",
        "Removes multicollinearity\n",
        "\n",
        "Produces orthogonal (uncorrelated) components\n",
        "\n",
        "Components are often not directly interpretable\n",
        "\n",
        "Feature Selection\n",
        "\n",
        "Feature selection is the process of choosing a subset of the original features without transforming them.\n",
        "\n",
        "The goal is to retain the most relevant features and remove redundant or irrelevant ones.\n",
        "\n",
        "Types of Feature Selection\n",
        "\n",
        "Filter methods (correlation, chi-square, ANOVA)\n",
        "\n",
        "Wrapper methods (forward selection, backward elimination)\n",
        "\n",
        "Embedded methods (Lasso, decision tree feature importance)"
      ],
      "metadata": {
        "id": "1yjuxZj5gXjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Answer: Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are the mathematical foundation that determine how the data is transformed and how much information is retained after dimensionality reduction.\n",
        "\n",
        "Eigenvectors in PCA\n",
        "\n",
        "An eigenvector represents a direction in the feature space along which the data varies the most.\n",
        "\n",
        "Role in PCA\n",
        "\n",
        "Each eigenvector corresponds to a principal component.\n",
        "\n",
        "It defines the new axis onto which the original data is projected.\n",
        "\n",
        "Eigenvectors are orthogonal (perpendicular) to each other, ensuring no redundancy.\n",
        "\n",
        "They are linear combinations of original features.\n",
        "\n",
        "Intuition\n",
        "\n",
        "Think of eigenvectors as the directions of maximum spread in the data cloud.\n",
        "\n",
        "Eigenvalues in PCA\n",
        "\n",
        "An eigenvalue indicates the amount of variance captured along its corresponding eigenvector.\n",
        "\n",
        "Role in PCA\n",
        "\n",
        "Larger eigenvalue → more information (variance) captured.\n",
        "\n",
        "Smaller eigenvalue → less useful component.\n",
        "\n",
        "Eigenvalues help rank principal components.\n",
        "\n",
        "Intuition\n",
        "\n",
        "Eigenvalues tell us how important each direction (eigenvector) is.\n",
        "\n",
        "Mathematical Context (Simplified)\n",
        "\n",
        "PCA computes eigenvectors and eigenvalues of the covariance matrix (or correlation matrix).\n",
        "\n",
        "If Σv = λv:\n",
        "\n",
        "v = eigenvector (principal component direction)\n",
        "\n",
        "λ = eigenvalue (variance along that direction)\n",
        "\n",
        "Why Eigenvalues and Eigenvectors Are Important in PCA\n",
        "\n",
        "Define New Feature Space\n",
        "\n",
        "Eigenvectors determine the axes of the reduced-dimensional space.\n",
        "\n",
        "Dimensionality Reduction\n",
        "\n",
        "Eigenvalues help decide how many principal components to keep.\n",
        "\n",
        "Variance Preservation\n",
        "\n",
        "Selecting components with the largest eigenvalues preserves maximum information.\n",
        "\n",
        "Noise Reduction\n",
        "\n",
        "Components with small eigenvalues often represent noise and can be discarded.\n",
        "\n",
        "Explained Variance Ratio\n",
        "\n",
        "Eigenvalues are used to compute the percentage of variance explained by each component."
      ],
      "metadata": {
        "id": "Hhv2ZaK3gXhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "\n",
        "Answer:  How KNN and PCA Complement Each Other in a Single Pipeline\n",
        "\n",
        "KNN and PCA are often combined in a single machine learning pipeline because PCA directly addresses the key weaknesses of KNN. Together, they improve accuracy, efficiency, and robustness, especially for high-dimensional data.\n",
        "\n",
        "Core Idea\n",
        "\n",
        "PCA reduces dimensionality and removes redundancy by projecting data into a lower-dimensional, uncorrelated feature space.\n",
        "\n",
        "KNN then performs distance-based prediction more effectively in this reduced space.\n",
        "\n",
        "In short: PCA prepares the feature space; KNN performs the prediction.\n",
        "\n",
        "Why PCA Improves KNN Performance\n",
        "1. Mitigates the Curse of Dimensionality\n",
        "\n",
        "KNN relies on distance metrics, which degrade in high dimensions.\n",
        "\n",
        "PCA reduces the number of features while preserving variance.\n",
        "\n",
        "Distances become more meaningful, improving neighbor selection.\n",
        "\n",
        "2. Removes Multicollinearity\n",
        "\n",
        "Highly correlated features distort distance calculations.\n",
        "\n",
        "PCA transforms correlated features into orthogonal components.\n",
        "\n",
        "KNN benefits from unbiased distance computation.\n",
        "\n",
        "3. Improves Computational Efficiency\n",
        "\n",
        "KNN has no training phase but is expensive at prediction time.\n",
        "\n",
        "Fewer dimensions → faster distance calculations → lower memory usage.\n",
        "\n",
        "4. Reduces Noise\n",
        "\n",
        "Components with small eigenvalues often represent noise.\n",
        "\n",
        "Removing them leads to cleaner neighborhoods and better predictions.\n",
        "\n",
        "Typical KNN + PCA Pipeline\n",
        "\n",
        "Standardize features\n",
        "\n",
        "Apply PCA (retain components explaining, e.g., 95% variance)\n",
        "\n",
        "Fit KNN on the transformed data\n",
        "\n",
        "Predict using reduced-dimension distances\n",
        "\n",
        "Practical Example\n",
        "Without PCA\n",
        "\n",
        "100 features\n",
        "\n",
        "Sparse data\n",
        "\n",
        "Nearest neighbors poorly defined\n",
        "\n",
        "High prediction latency\n",
        "\n",
        "With PCA\n",
        "\n",
        "Reduced to 15 principal components\n",
        "\n",
        "Compact, dense feature space\n",
        "\n",
        "More reliable neighbors\n",
        "\n",
        "Faster and more accurate predictions\n",
        "\n",
        "When This Combination Is Most Effective\n",
        "\n",
        "High-dimensional datasets\n",
        "\n",
        "Correlated or noisy features\n",
        "\n",
        "Distance-based models (KNN, K-Means)\n",
        "\n",
        "Image, text embeddings, gene expression data"
      ],
      "metadata": {
        "id": "NR0-A31BgXeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases. (Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:  KNN is a distance-based algorithm.\n",
        "The Wine dataset contains features with very different scales (e.g., alcohol vs proline).\n",
        "Without scaling, features with larger numeric ranges dominate distance calculations, leading to poor performance."
      ],
      "metadata": {
        "id": "69TB0hU3h7kM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMVMv0FhgUql",
        "outputId": "b3515ec3-ee20-44fb-d000-0df60f3e9aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8056\n",
            "Accuracy with scaling:    0.9722\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# KNN without scaling\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# KNN with scaling (pipeline)\n",
        "knn_scaled_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "knn_scaled_pipeline.fit(X_train, y_train)\n",
        "y_pred_scaled = knn_scaled_pipeline.predict(X_test)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {acc_scaled:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component. (Include your Python code)\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Standardize features (required for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"Principal Component {i}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNM_I4ftiaOv",
        "outputId": "4fbb777a-4c08-497d-c121-7ff9d5300687"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset. (Include your Python code)\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# KNN on ORIGINAL (SCALED) DATA\n",
        "# -------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -------------------------------\n",
        "# PCA (Top 2 Components)\n",
        "# -------------------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# -------------------------------\n",
        "# Results\n",
        "# -------------------------------\n",
        "print(f\"Accuracy with original features: {acc_original:.4f}\")\n",
        "print(f\"Accuracy with PCA (2 components): {acc_pca:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjL5bceHihBR",
        "outputId": "f41fa134-abd0-4abd-db97-775fd3b16540"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with original features: 0.9722\n",
            "Accuracy with PCA (2 components): 0.9167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results. (Include your Python code )\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -------------------------------\n",
        "# KNN with Euclidean distance\n",
        "# -------------------------------\n",
        "knn_euclidean = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='euclidean'\n",
        ")\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -------------------------------\n",
        "# KNN with Manhattan distance\n",
        "# -------------------------------\n",
        "knn_manhattan = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='manhattan'\n",
        ")\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# -------------------------------\n",
        "# Results\n",
        "# -------------------------------\n",
        "print(f\"Accuracy with Euclidean distance: {acc_euclidean:.4f}\")\n",
        "print(f\"Accuracy with Manhattan distance: {acc_manhattan:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj8vQU3riwaj",
        "outputId": "8b95ae50-78f6-42e1-9c84-776622ac4eb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9722\n",
            "Accuracy with Manhattan distance: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit. Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data (Include your Python code\n",
        "\n",
        "Answer:  Gene expression datasets typically have thousands of genes (features) but very few patient samples. This setting is highly prone to overfitting, poor generalization, and unstable distance calculations.\n",
        "\n",
        "A PCA → KNN pipeline directly addresses these challenges.\n",
        "\n",
        "1. Using PCA to Reduce Dimensionality\n",
        "Why PCA is appropriate\n",
        "\n",
        "Gene expression features are highly correlated\n",
        "\n",
        "PCA:\n",
        "\n",
        "Projects data into orthogonal components\n",
        "\n",
        "Preserves maximum biological variance\n",
        "\n",
        "Removes noise and redundancy\n",
        "\n",
        "Mitigates the curse of dimensionality\n",
        "\n",
        "How it is applied\n",
        "\n",
        "Standardize gene expression values\n",
        "\n",
        "Apply PCA on the training set only\n",
        "\n",
        "Transform both train and test data using learned components\n",
        "\n",
        "2. Deciding How Many Components to Keep\n",
        "\n",
        "We retain components based on explained variance, not an arbitrary number.\n",
        "\n",
        "Common biomedical practice\n",
        "\n",
        "Keep components explaining 90–95% cumulative variance\n",
        "\n",
        "This balances:\n",
        "\n",
        "Information retention\n",
        "\n",
        "Noise reduction\n",
        "\n",
        "Model stability\n",
        "\n",
        "Decision rule\n",
        "\n",
        "Choose the smallest number of components such that cumulative explained variance ≥ 95%\n",
        "\n",
        "This avoids overfitting while preserving disease-related signal.\n",
        "\n",
        "3. Using KNN After PCA\n",
        "Why KNN?\n",
        "\n",
        "Non-parametric (no assumptions about gene distributions)\n",
        "\n",
        "Works well in low-dimensional, denoised spaces\n",
        "\n",
        "Naturally captures patient similarity patterns\n",
        "\n",
        "Why after PCA?\n",
        "\n",
        "PCA ensures:\n",
        "\n",
        "Meaningful distance calculations\n",
        "\n",
        "Reduced computational cost\n",
        "\n",
        "Improved neighborhood quality\n",
        "\n",
        "KNN is trained on PCA-transformed features, not raw gene counts.\n",
        "\n",
        "4. Model Evaluation Strategy\n",
        "\n",
        "Because biomedical datasets are small, evaluation must be rigorous.\n",
        "\n",
        "Metrics used\n",
        "\n",
        "Accuracy (overall performance)\n",
        "\n",
        "Confusion matrix (class-wise errors)\n",
        "\n",
        "Cross-validation (stability across splits)\n",
        "\n",
        "Validation principles\n",
        "\n",
        "PCA fitted only on training folds (prevents data leakage)\n",
        "\n",
        "Stratified splits to preserve cancer class balance\n",
        "\n",
        "5. Justifying This Pipeline to Stakeholders\n",
        "Why this is robust for real-world biomedical data\n",
        "\n",
        "Scientific validity\n",
        "\n",
        "PCA captures dominant biological variation\n",
        "\n",
        "Reduces noise from irrelevant or low-expression genes\n",
        "\n",
        "Statistical reliability\n",
        "\n",
        "Prevents overfitting in small-sample, high-feature settings\n",
        "\n",
        "Improves generalization to unseen patients\n",
        "\n",
        "Operational feasibility\n",
        "\n",
        "Faster inference\n",
        "\n",
        "Lower memory footprint\n",
        "\n",
        "Easy to retrain when new samples arrive\n",
        "\n",
        "Clinical interpretability\n",
        "\n",
        "Variance-based dimensionality reduction is widely accepted\n",
        "\n",
        "Model decisions are based on patient similarity in latent biological space\n",
        "\n",
        "This pipeline aligns with best practices in bioinformatics and translational medicine."
      ],
      "metadata": {
        "id": "9wsiooOcjKoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine  # placeholder for gene expression data\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load example dataset (replace with gene expression matrix)\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# PCA + KNN pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),  # retain 95% variance\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5, metric='euclidean'))\n",
        "])\n",
        "\n",
        "# Train model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSGDpn6xjB47",
        "outputId": "87568ba6-e55d-4559-b76c-3e62590fcaff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9722\n",
            "Confusion Matrix:\n",
            " [[12  0  0]\n",
            " [ 0 13  1]\n",
            " [ 0  0 10]]\n",
            "Cross-Validation Accuracy: 0.9495 ± 0.0329\n"
          ]
        }
      ]
    }
  ]
}