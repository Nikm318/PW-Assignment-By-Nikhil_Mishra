{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 :  What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer :\n",
        "Information Gain (IG) is a metric from information theory that measures how much uncertainty (impurity) in a dataset is reduced after splitting it on a particular feature. In decision trees, it is used to decide which attribute should be chosen at each node for splitting the data.\n",
        "\n",
        "1. Conceptual Meaning\n",
        "\n",
        "Decision trees aim to create pure subsets, where records belong to a single class.\n",
        "\n",
        "Information Gain quantifies how much information a feature provides about the class label.\n",
        "\n",
        "The feature with the highest Information Gain is selected for the split.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "Information Gain tells us which feature best separates the data into meaningful classes.\n",
        "\n",
        "2. Entropy: The Foundation of Information Gain\n",
        "\n",
        "Information Gain is based on Entropy, which measures randomness or impurity.\n",
        "\n",
        "Entropy Formula\n",
        "ğ¸\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        "ğ‘Ÿ\n",
        "ğ‘œ\n",
        "ğ‘\n",
        "ğ‘¦\n",
        "(\n",
        "ğ‘†\n",
        ")\n",
        "=âˆ’\n",
        "âˆ‘ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘\n",
        "ğ‘\n",
        "ğ‘–\n",
        "log\n",
        "â¡\n",
        "2\n",
        "(\n",
        "ğ‘\n",
        "ğ‘–\n",
        ")\n",
        "Entropy(S)=âˆ’\n",
        "i=1\n",
        "âˆ‘\n",
        "c\n",
        "\tâ€‹\n",
        "\n",
        "p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "log\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "(p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘†\n",
        "S = dataset\n",
        "\n",
        "ğ‘\n",
        "ğ‘–\n",
        "p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        " = proportion of class\n",
        "ğ‘–\n",
        "i\n",
        "\n",
        "ğ‘\n",
        "c = number of classes\n",
        "\n",
        "Interpretation\n",
        "\n",
        "Entropy = 0 â†’ perfectly pure (all samples belong to one class)\n",
        "\n",
        "Higher entropy â†’ more disorder or uncertainty\n",
        "\n",
        "3. Information Gain Formula\n",
        "ğ¼\n",
        "ğº\n",
        "(\n",
        "ğ‘†\n",
        ",\n",
        "ğ´\n",
        ")\n",
        "=\n",
        "ğ¸\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        "ğ‘Ÿ\n",
        "ğ‘œ\n",
        "ğ‘\n",
        "ğ‘¦\n",
        "(\n",
        "ğ‘†\n",
        ")\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘£\n",
        "âˆˆ\n",
        "ğ‘‰\n",
        "ğ‘\n",
        "ğ‘™\n",
        "ğ‘¢\n",
        "ğ‘’\n",
        "ğ‘ \n",
        "(\n",
        "ğ´\n",
        ")\n",
        "âˆ£\n",
        "ğ‘†\n",
        "ğ‘£\n",
        "âˆ£\n",
        "âˆ£\n",
        "ğ‘†\n",
        "âˆ£\n",
        "Ã—\n",
        "ğ¸\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        "ğ‘Ÿ\n",
        "ğ‘œ\n",
        "ğ‘\n",
        "ğ‘¦\n",
        "(\n",
        "ğ‘†\n",
        "ğ‘£\n",
        ")\n",
        "IG(S,A)=Entropy(S)âˆ’\n",
        "vâˆˆValues(A)\n",
        "âˆ‘\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£Sâˆ£\n",
        "âˆ£S\n",
        "v\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£\n",
        "\tâ€‹\n",
        "\n",
        "Ã—Entropy(S\n",
        "v\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘†\n",
        "S = original dataset\n",
        "\n",
        "ğ´\n",
        "A = attribute (feature)\n",
        "\n",
        "ğ‘†\n",
        "ğ‘£\n",
        "S\n",
        "v\n",
        "\tâ€‹\n",
        "\n",
        " = subset of\n",
        "ğ‘†\n",
        "S for value\n",
        "ğ‘£\n",
        "v of attribute\n",
        "ğ´\n",
        "A\n",
        "\n",
        "4. How Information Gain is Used in Decision Trees\n",
        "Step-by-step process:\n",
        "\n",
        "Compute entropy of the entire dataset\n",
        "\n",
        "For each feature:\n",
        "\n",
        "Split the data based on that feature\n",
        "\n",
        "Compute the weighted entropy of the subsets\n",
        "\n",
        "Calculate Information Gain for each feature\n",
        "\n",
        "Choose the feature with maximum Information Gain\n",
        "\n",
        "Repeat recursively until:\n",
        "\n",
        "All nodes are pure, or\n",
        "\n",
        "A stopping criterion is met\n",
        "\n",
        "This approach is used in ID3 and C4.5 decision tree algorithms.\n",
        "\n",
        "5. Simple Example\n",
        "\n",
        "Suppose we are predicting whether to Play Cricket based on Weather.\n",
        "\n",
        "Entropy before split = 0.94\n",
        "\n",
        "Entropy after splitting on Weather = 0.69\n",
        "\n",
        "ğ¼\n",
        "ğº\n",
        "(\n",
        "ğ‘Š\n",
        "ğ‘’\n",
        "ğ‘\n",
        "ğ‘¡\n",
        "â„\n",
        "ğ‘’\n",
        "ğ‘Ÿ\n",
        ")\n",
        "=\n",
        "0.94\n",
        "âˆ’\n",
        "0.69\n",
        "=\n",
        "0.25\n",
        "IG(Weather)=0.94âˆ’0.69=0.25\n",
        "\n",
        "This means Weather reduces uncertainty by 0.25 bits, making it a good splitting attribute.\n",
        "\n",
        "6. Advantages and Limitations\n",
        "Advantages\n",
        "\n",
        "Easy to understand and interpret\n",
        "\n",
        "Works well for categorical data\n",
        "\n",
        "Strong theoretical foundation\n",
        "\n",
        "Limitations\n",
        "\n",
        "Biased toward attributes with many unique values\n",
        "\n",
        "Less effective for continuous features (needs discretization)"
      ],
      "metadata": {
        "id": "amqOaxE6ujWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy? Hint: Directly compares the two main impurity measures, highlighting strengths, weaknesses, and appropriate use cases.\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gini Impurity and Entropy are the two most commonly used impurity measures in decision tree algorithms. Both quantify how â€œmixedâ€ a node is, but they differ in mathematical formulation, interpretability, computational cost, and practical use.\n",
        "\n",
        "1. Definition and Formula\n",
        "Measure\tFormula\n",
        "Entropy\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘\n",
        "ğ‘\n",
        "ğ‘–\n",
        "log\n",
        "â¡\n",
        "2\n",
        "(\n",
        "ğ‘\n",
        "ğ‘–\n",
        ")\n",
        "âˆ’âˆ‘\n",
        "i=1\n",
        "c\n",
        "\tâ€‹\n",
        "\n",
        "p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "log\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "(p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "Gini Impurity\n",
        "1\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘\n",
        "ğ‘\n",
        "ğ‘–\n",
        "2\n",
        "1âˆ’âˆ‘\n",
        "i=1\n",
        "c\n",
        "\tâ€‹\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘\n",
        "ğ‘–\n",
        "p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        " = proportion of class\n",
        "ğ‘–\n",
        "i\n",
        "\n",
        "ğ‘\n",
        "c = number of classes\n",
        "\n",
        "2. Conceptual Difference\n",
        "Aspect\tEntropy\tGini Impurity\n",
        "Theoretical basis\tInformation Theory\tProbability / Misclassification\n",
        "Interpretation\tMeasures uncertainty or surprise\tMeasures probability of incorrect classification\n",
        "Unit\tBits of information\tProbability\n",
        "Pure node value\t0\t0\n",
        "\n",
        "3. Sensitivity to Class Distribution\n",
        "\n",
        "Entropy\n",
        "\n",
        "More sensitive to changes in class probabilities\n",
        "\n",
        "Penalizes imbalanced splits more heavily\n",
        "\n",
        "Prefers more balanced partitions\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Less sensitive to small probability changes\n",
        "\n",
        "Tends to isolate the most frequent class faster\n",
        "\n",
        "Can create slightly less balanced trees\n",
        "\n",
        "4. Computational Efficiency\n",
        "Aspect\tEntropy\tGini\n",
        "Logarithmic computation\tRequired\tNot required\n",
        "Speed\tSlower\tFaster\n",
        "Scalability\tLess efficient for large datasets\tMore efficient for large datasets\n",
        "\n",
        "This is why CART algorithms typically prefer Gini.\n",
        "\n",
        "5. Impact on Tree Structure\n",
        "\n",
        "Entropy\n",
        "\n",
        "May generate deeper, more complex trees\n",
        "\n",
        "Often results in marginally better splits in small datasets\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Produces simpler trees\n",
        "\n",
        "Often reaches a good split faster\n",
        "\n",
        "In practice, both often yield very similar trees.\n",
        "\n",
        "6. Typical Algorithms Using Them\n",
        "Algorithm\tImpurity Measure\n",
        "ID3\tEntropy\n",
        "C4.5\tEntropy (Gain Ratio)\n",
        "CART\tGini Impurity\n",
        "\n",
        "\n",
        "7. When to Use Which?\n",
        "Use Gini Impurity when:\n",
        "\n",
        "You need faster training\n",
        "\n",
        "Dataset is large\n",
        "\n",
        "Slight approximation is acceptable\n",
        "\n",
        "Use Entropy when:\n",
        "\n",
        "You want better theoretical interpretability\n",
        "\n",
        "Dataset is small or moderate\n",
        "\n",
        "Class balance is critical"
      ],
      "metadata": {
        "id": "9QG7g_4eujS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:  \n",
        "\n",
        "Pre-pruning (also called early stopping) in decision trees is a technique used to prevent overfitting by stopping the growth of the tree at an early stage, before it perfectly fits the training data.\n",
        "\n",
        "Instead of allowing the tree to grow fully and pruning it afterward, pre-pruning restricts tree expansion during construction based on predefined criteria.\n",
        "\n",
        "1. Why Pre-Pruning Is Needed\n",
        "\n",
        "Decision trees can:\n",
        "\n",
        "Grow very deep\n",
        "\n",
        "Memorize noise in the training data\n",
        "\n",
        "Perform poorly on unseen (test) data\n",
        "\n",
        "Pre-pruning addresses this by controlling model complexity upfront, improving generalization.\n",
        "\n",
        "2. How Pre-Pruning Works\n",
        "\n",
        "While building the decision tree, splitting at a node is stopped if certain conditions are met, such as:\n",
        "\n",
        "The split does not significantly reduce impurity\n",
        "\n",
        "The node has too few samples\n",
        "\n",
        "The tree has reached a maximum depth\n",
        "\n",
        "If splitting stops, the node becomes a leaf node, and a class label (or mean value in regression) is assigned.\n",
        "\n",
        "| Technique                        | Description                                          |\n",
        "| -------------------------------- | ---------------------------------------------------- |\n",
        "| **Maximum Depth**                | Limits how deep the tree can grow                    |\n",
        "| **Minimum Samples Split**        | Requires a minimum number of samples to split a node |\n",
        "| **Minimum Samples Leaf**         | Ensures leaf nodes contain enough data               |\n",
        "| **Minimum Impurity Decrease**    | Split only if impurity reduction exceeds a threshold |\n",
        "| **Maximum Number of Leaf Nodes** | Restricts total leaf nodes                           |\n",
        "\n",
        "\n",
        "4. Example\n",
        "\n",
        "Suppose a node has 20 samples:\n",
        "\n",
        "A potential split reduces Gini impurity from 0.48 to 0.46\n",
        "\n",
        "Minimum required reduction = 0.05\n",
        "\n",
        "Since the improvement is too small, the split is rejected, and the node becomes a leaf.\n",
        "\n",
        "5. Advantages of Pre-Pruning\n",
        "\n",
        "Reduces overfitting\n",
        "\n",
        "Faster training\n",
        "\n",
        "Produces simpler, more interpretable trees\n",
        "\n",
        "Less memory usage\n",
        "\n",
        "6. Limitations of Pre-Pruning\n",
        "\n",
        "Risk of underfitting\n",
        "\n",
        "Requires careful tuning of hyperparameters\n",
        "\n",
        "May stop growth before discovering useful deeper patterns\n",
        "\n",
        "\n",
        "| Aspect             | Pre-Pruning              | Post-Pruning                    |\n",
        "| ------------------ | ------------------------ | ------------------------------- |\n",
        "| When applied       | During tree construction | After full tree is grown        |\n",
        "| Computational cost | Lower                    | Higher                          |\n",
        "| Risk               | Underfitting             | Overfitting control is stronger |\n",
        "| Flexibility        | Limited                  | More flexible                   |\n"
      ],
      "metadata": {
        "id": "QCSERdByujQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ2L9ScwuYhm",
        "outputId": "6da07bed-23cf-4382-c342-536e4260ca0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n"
          ]
        }
      ],
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical). Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load sample dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert feature names for readability\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create Decision Tree Classifier using Gini Impurity\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = dt_model.feature_importances_\n",
        "\n",
        "# Display feature importances in tabular form\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "print(feature_importance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression that works by finding an optimal decision boundary (hyperplane) which best separates data points of different classes.\n",
        "\n",
        "The core idea of SVM is to maximize the marginâ€”the distance between the decision boundary and the nearest data points from each class, called support vectors.\n",
        "\n",
        "1. Core Concepts of SVM\n",
        "(a) Hyperplane\n",
        "\n",
        "A hyperplane is a decision boundary that separates classes.\n",
        "\n",
        "In:\n",
        "\n",
        "2D â†’ a line\n",
        "\n",
        "3D â†’ a plane\n",
        "\n",
        "nD â†’ an (nâˆ’1)-dimensional hyperplane\n",
        "\n",
        "(b) Margin\n",
        "\n",
        "Margin is the distance between the hyperplane and the closest data points.\n",
        "\n",
        "SVM chooses the hyperplane with the maximum margin.\n",
        "\n",
        "(c) Support Vectors\n",
        "\n",
        "Data points that lie closest to the hyperplane.\n",
        "\n",
        "They define the position and orientation of the decision boundary.\n",
        "\n",
        "2. Types of SVM\n",
        "(a) Linear SVM\n",
        "\n",
        "Used when data is linearly separable.\n",
        "\n",
        "Decision boundary is a straight line (or plane).\n",
        "\n",
        "(b) Non-Linear SVM\n",
        "\n",
        "Used when data is not linearly separable.\n",
        "\n",
        "Uses the kernel trick to map data into a higher-dimensional space.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "Radial Basis Function (RBF)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "3. Mathematical Intuition (High-Level)\n",
        "\n",
        "SVM solves an optimization problem:\n",
        "\n",
        "Objective:\n",
        "Maximize margin\n",
        "or equivalently minimize\n",
        "âˆ£\n",
        "âˆ£\n",
        "ğ‘¤\n",
        "âˆ£\n",
        "âˆ£\n",
        "âˆ£âˆ£wâˆ£âˆ£\n",
        "\n",
        "Subject to:\n",
        "Correct classification (or controlled misclassification using slack variables)\n",
        "\n",
        "This makes SVM a maximum-margin classifier.\n",
        "\n",
        "4. Advantages of SVM\n",
        "\n",
        "Effective in high-dimensional spaces\n",
        "\n",
        "Works well with small to medium-sized datasets\n",
        "\n",
        "Robust to overfitting due to margin maximization\n",
        "\n",
        "Flexible through kernel functions\n",
        "\n",
        "5. Limitations of SVM\n",
        "\n",
        "Computationally expensive for very large datasets\n",
        "\n",
        "Kernel and parameter selection can be complex\n",
        "\n",
        "Less interpretable compared to decision trees\n",
        "\n",
        "6. Common Use Cases\n",
        "\n",
        "Text classification (spam detection)\n",
        "\n",
        "Image recognition\n",
        "\n",
        "Bioinformatics (gene classification)\n",
        "\n",
        "Handwritten digit recognition"
      ],
      "metadata": {
        "id": "2BzNQYwQxEhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:  What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Kernel Trick in Support Vector Machines (SVMs) is a technique that enables SVMs to handle non-linearly separable data by implicitly mapping input features into a higher-dimensional feature space, where a linear separating hyperplane can be constructedâ€”without explicitly computing that transformation.\n",
        "\n",
        "1. Why the Kernel Trick Is Needed\n",
        "\n",
        "Many real-world datasets are not linearly separable in their original feature space.\n",
        "\n",
        "Explicitly transforming data into higher dimensions is:\n",
        "\n",
        "Computationally expensive\n",
        "\n",
        "Memory intensive\n",
        "\n",
        "The kernel trick avoids this by computing inner products in the transformed space directly, using a kernel function.\n",
        "\n",
        "2. Core Idea\n",
        "\n",
        "Instead of computing:\n",
        "\n",
        "ğœ™\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "Ï•(x)\n",
        "\n",
        "SVM computes:\n",
        "\n",
        "ğ¾\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        ",\n",
        "ğ‘¥\n",
        "ğ‘—\n",
        ")\n",
        "=\n",
        "ğœ™\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        ")\n",
        "â‹…\n",
        "ğœ™\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘—\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ",x\n",
        "j\n",
        "\tâ€‹\n",
        "\n",
        ")=Ï•(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")â‹…Ï•(x\n",
        "j\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "ğœ™\n",
        "Ï• = (implicit) feature mapping function\n",
        "\n",
        "ğ¾\n",
        "K = kernel function\n",
        "\n",
        "This allows SVM to learn non-linear decision boundaries efficiently.\n",
        "\n",
        "3. Visual Intuition\n",
        "\n",
        "In 2D space â†’ classes overlap (non-linear)\n",
        "\n",
        "Map to higher dimension â†’ classes become linearly separable\n",
        "\n",
        "Draw a linear hyperplane in higher space\n",
        "\n",
        "Project it back â†’ non-linear boundary in original space.\n",
        "\n",
        "| Kernel             | Formula                         | Use Case                     |     |   |       |                                 |\n",
        "| ------------------ | ------------------------------- | ---------------------------- | --- | - | ----- | ------------------------------- |\n",
        "| **Linear**         | ( K(x,y) = x \\cdot y )          | Linearly separable data      |     |   |       |                                 |\n",
        "| **Polynomial**     | ( (x \\cdot y + c)^d )           | Feature interaction modeling |     |   |       |                                 |\n",
        "| **RBF (Gaussian)** | ( \\exp(-\\gamma                  |                              | x-y |   | ^2) ) | Most common, complex boundaries |\n",
        "| **Sigmoid**        | ( \\tanh(\\alpha x \\cdot y + c) ) | Neural-network-like behavior |     |   |       |                                 |\n",
        "\n",
        "\n",
        "5. Advantages of the Kernel Trick\n",
        "\n",
        "Handles complex, non-linear patterns\n",
        "\n",
        "Avoids explicit high-dimensional computation\n",
        "\n",
        "Computationally efficient\n",
        "\n",
        "Highly flexible via kernel choice\n",
        "\n",
        "6. Limitations\n",
        "\n",
        "Kernel selection is non-trivial\n",
        "\n",
        "Parameter tuning (C, Î³, degree) is critical\n",
        "\n",
        "Poor choice of kernel can lead to overfitting\n",
        "\n",
        "7. Example (Intuitive)\n",
        "\n",
        "Consider data arranged in concentric circles:\n",
        "\n",
        "Linear classifier fails\n",
        "\n",
        "RBF kernel maps data into higher dimensions\n",
        "\n",
        "Linear separation becomes possible"
      ],
      "metadata": {
        "id": "6CafYGtrxajN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7:  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies. Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 1. SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# 2. SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN2F-CkNxtGj",
        "outputId": "13c0c6ba-b9e7-4380-8d33-5978107662da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9722222222222222\n",
            "RBF SVM Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the NaÃ¯ve Bayes classifier, and why is it called \"NaÃ¯ve\"?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The NaÃ¯ve Bayes classifier is a supervised probabilistic machine learning algorithm used primarily for classification tasks, based on Bayesâ€™ Theorem and a strong assumption of conditional independence among features.\n",
        "\n",
        "It is widely used in text classification, spam filtering, and sentiment analysis due to its simplicity and efficiency.\n",
        "\n",
        "1. Core Idea: Bayesâ€™ Theorem\n",
        "\n",
        "NaÃ¯ve Bayes computes the posterior probability of a class given input features:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "âˆ£\n",
        "ğ‘‹\n",
        ")\n",
        "=\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        ")\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        ")\n",
        "P(Câˆ£X)=\n",
        "P(X)\n",
        "P(Xâˆ£C)P(C)\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ¶\n",
        "C = class label\n",
        "\n",
        "ğ‘‹\n",
        "=\n",
        "(\n",
        "ğ‘¥\n",
        "1\n",
        ",\n",
        "ğ‘¥\n",
        "2\n",
        ",\n",
        "â€¦\n",
        ",\n",
        "ğ‘¥\n",
        "ğ‘›\n",
        ")\n",
        "X=(x\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        ",x\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        ",â€¦,x\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        ") = feature vector\n",
        "\n",
        "Since\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        ")\n",
        "P(X) is constant for all classes, classification is based on:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "âˆ£\n",
        "ğ‘‹\n",
        ")\n",
        "âˆ\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        ")\n",
        "âˆ\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "P(Câˆ£X)âˆP(C)\n",
        "i=1\n",
        "âˆ\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)\n",
        "2. Why It Is Called â€œNaÃ¯veâ€\n",
        "\n",
        "It is called â€œNaÃ¯veâ€ because it makes a strong and often unrealistic assumption:\n",
        "\n",
        "All features are conditionally independent given the class label.\n",
        "\n",
        "That is:\n",
        "\n",
        "Each feature contributes independently to the final prediction\n",
        "\n",
        "Correlations among features are ignored\n",
        "\n",
        "Example\n",
        "\n",
        "In email spam detection:\n",
        "\n",
        "Words like â€œfreeâ€ and â€œofferâ€ are treated as independent\n",
        "\n",
        "In reality, they are often correlated\n",
        "\n",
        "Despite this simplification, NaÃ¯ve Bayes performs surprisingly well in many real-world problems.\n",
        "\n",
        "3. How Classification Works\n",
        "\n",
        "For each class:\n",
        "\n",
        "Compute prior probability\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        ")\n",
        "P(C)\n",
        "\n",
        "Compute likelihoods\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)\n",
        "\n",
        "Multiply probabilities to get posterior score\n",
        "\n",
        "Choose class with highest posterior probability.\n",
        "\n",
        "| Type                        | Data Type       | Use Case             |\n",
        "| --------------------------- | --------------- | -------------------- |\n",
        "| **Gaussian NaÃ¯ve Bayes**    | Continuous      | Medical, sensor data |\n",
        "| **Multinomial NaÃ¯ve Bayes** | Discrete counts | Text classification  |\n",
        "| **Bernoulli NaÃ¯ve Bayes**   | Binary features | Spam detection       |\n",
        "\n",
        "\n",
        "5. Advantages\n",
        "\n",
        "Simple and fast to train\n",
        "\n",
        "Works well with high-dimensional data\n",
        "\n",
        "Requires small training datasets\n",
        "\n",
        "Handles irrelevant features well\n",
        "\n",
        "6. Limitations\n",
        "\n",
        "Independence assumption rarely holds in practice\n",
        "\n",
        "Zero-frequency problem (handled via smoothing)\n",
        "\n",
        "Less accurate than complex models when feature dependency is strong"
      ],
      "metadata": {
        "id": "TyoXFF3fx7oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian NaÃ¯ve Bayes, Multinomial NaÃ¯ve Bayes, and Bernoulli NaÃ¯ve Bayes\n",
        "\n",
        "Answer:\n",
        "\n",
        "NaÃ¯ve Bayes classifiers all rely on Bayesâ€™ Theorem and the conditional independence assumption, but they differ in how they model the distribution of features. The choice among Gaussian, Multinomial, and Bernoulli NaÃ¯ve Bayes depends primarily on the type of input data.\n",
        "\n",
        "1. Gaussian NaÃ¯ve Bayes (GNB)\n",
        "Feature Assumption\n",
        "\n",
        "Features are continuous and follow a normal (Gaussian) distribution.\n",
        "\n",
        "Likelihood Model\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ğœ‹\n",
        "ğœ\n",
        "ğ¶\n",
        "2\n",
        "exp\n",
        "â¡\n",
        "(\n",
        "âˆ’\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğœ‡\n",
        "ğ¶\n",
        ")\n",
        "2\n",
        "2\n",
        "ğœ\n",
        "ğ¶\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)=\n",
        "2Ï€Ïƒ\n",
        "C\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "\tâ€‹\n",
        "\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "exp(âˆ’\n",
        "2Ïƒ\n",
        "C\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’Î¼\n",
        "C\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "Typical Use Cases\n",
        "\n",
        "Medical measurements\n",
        "\n",
        "Sensor data\n",
        "\n",
        "Real-valued features\n",
        "\n",
        "Key Characteristics\n",
        "\n",
        "Estimates mean (Î¼) and variance (ÏƒÂ²) per class\n",
        "\n",
        "No discretization required\n",
        "\n",
        "2. Multinomial NaÃ¯ve Bayes (MNB)\n",
        "Feature Assumption\n",
        "\n",
        "Features are discrete counts (non-negative integers).\n",
        "\n",
        "Likelihood Model\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "âˆ\n",
        "âˆ\n",
        "ğ‘–\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¤\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "P(xâˆ£C)âˆ\n",
        "i\n",
        "âˆ\n",
        "\tâ€‹\n",
        "\n",
        "P(w\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)\n",
        "x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "Typical Use Cases\n",
        "\n",
        "Text classification\n",
        "\n",
        "Document categorization\n",
        "\n",
        "Spam detection using word frequencies\n",
        "\n",
        "Key Characteristics\n",
        "\n",
        "Works with term frequencies or TF-IDF\n",
        "\n",
        "Sensitive to feature counts\n",
        "\n",
        "Uses Laplace smoothing to handle zero counts\n",
        "\n",
        "3. Bernoulli NaÃ¯ve Bayes (BNB)\n",
        "Feature Assumption\n",
        "\n",
        "Features are binary (0 or 1).\n",
        "\n",
        "Likelihood Model\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "=\n",
        "ğ‘\n",
        "ğ‘–\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ğ‘\n",
        "ğ‘–\n",
        ")\n",
        "1\n",
        "âˆ’\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)=p\n",
        "i\n",
        "x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "\tâ€‹\n",
        "\n",
        "(1âˆ’p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "1âˆ’x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "Typical Use Cases\n",
        "\n",
        "Binary text features (word present or absent)\n",
        "\n",
        "Email spam classification\n",
        "\n",
        "Key Characteristics\n",
        "\n",
        "Considers presence/absence, not frequency\n",
        "\n",
        "| Aspect                 | Gaussian NB       | Multinomial NB     | Bernoulli NB         |\n",
        "| ---------------------- | ----------------- | ------------------ | -------------------- |\n",
        "| Feature type           | Continuous        | Count-based        | Binary               |\n",
        "| Distribution assumed   | Normal (Gaussian) | Multinomial        | Bernoulli            |\n",
        "| Common domain          | Numerical data    | Text (word counts) | Text (word presence) |\n",
        "| Uses feature frequency | No                | Yes                | No                   |\n",
        "| Penalizes absence      | No                | No                 | Yes                  |\n",
        "\n",
        "\n",
        "5. Example to Clarify\n",
        "\n",
        "Consider text classification:\n",
        "\n",
        "Gaussian NB â†’ Uses average word length, sentence length\n",
        "\n",
        "Multinomial NB â†’ Uses how many times each word appears\n",
        "\n",
        "Bernoulli NB â†’ Uses whether a word appears or not\n",
        "\n",
        "6. When to Use Which?\n",
        "\n",
        "Use Gaussian NaÃ¯ve Bayes when features are continuous and normally distributed\n",
        "\n",
        "Use Multinomial NaÃ¯ve Bayes for text data with word counts or TF-IDF\n",
        "\n",
        "Use Bernoulli NaÃ¯ve Bayes for binary feature vectors"
      ],
      "metadata": {
        "id": "J8gjWdpzx7X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10:  Breast Cancer Dataset Write a Python program to train a Gaussian NaÃ¯ve Bayes classifier on the Breast Cancer dataset and evaluate accuracy. Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gaussian NaÃ¯ve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the result\n",
        "print(\"Gaussian NaÃ¯ve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDIzU11Cx1jE",
        "outputId": "fdd9dc32-0533-458e-9a7c-55f573c1cd00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian NaÃ¯ve Bayes Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GI-y4sdAzKHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}